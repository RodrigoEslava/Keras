{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigoEslava/Keras/blob/main/Deep_Learing_para_Bioinform%C3%A1tica_deep_learning_com_Keras_parte_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PMQcDR7bcGS",
        "outputId": "dbb59851-86e1-42e8-e919-fea6a94128b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-13 18:13:32--  https://webs.iiitd.edu.in/raghava/anticp2/pos_train_main\n",
            "Resolving webs.iiitd.edu.in (webs.iiitd.edu.in)... 103.25.231.42\n",
            "Connecting to webs.iiitd.edu.in (webs.iiitd.edu.in)|103.25.231.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14940 (15K)\n",
            "Saving to: ‘anticp.txt’\n",
            "\n",
            "anticp.txt          100%[===================>]  14.59K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-03-13 18:13:33 (366 KB/s) - ‘anticp.txt’ saved [14940/14940]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O anticp.txt 'https://webs.iiitd.edu.in/raghava/anticp2/pos_train_main'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for peptide in open('anticp.txt'):\n",
        "  peptide='$' + peptide.strip('\\n') + '@'\n",
        "  for i in range(0, len(peptide)-1):\n",
        "    X.append(peptide[:i+1])\n",
        "    y.append(peptide[i+1])"
      ],
      "metadata": {
        "id": "1unH1thDmQWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "metadata": {
        "id": "82hLKdUAnZpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "vvPcr9gloJ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gybIJOBBofY-",
        "outputId": "2f0cabf9-9d07-453b-801c-bd9f2b0df1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'k': 1,\n",
              " 'g': 2,\n",
              " 'l': 3,\n",
              " 'a': 4,\n",
              " '$': 5,\n",
              " 'c': 6,\n",
              " 'f': 7,\n",
              " 'i': 8,\n",
              " 's': 9,\n",
              " 'r': 10,\n",
              " 'v': 11,\n",
              " 'p': 12,\n",
              " 't': 13,\n",
              " 'n': 14,\n",
              " 'w': 15,\n",
              " 'e': 16,\n",
              " 'd': 17,\n",
              " 'h': 18,\n",
              " 'y': 19,\n",
              " 'q': 20,\n",
              " 'm': 21}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_tokens_padded = pad_sequences(X_train_tokens, maxlen=50)\n",
        "X_test_tokens_padded = pad_sequences(X_test_tokens, maxlen=50)"
      ],
      "metadata": {
        "id": "CAQ7okg4ou6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokens_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v70TOS6bpDIi",
        "outputId": "bb943c51-e8ba-4c90-c0e0-46ae894ed47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11205, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "\n",
        "y_train_encoded = to_categorical(le.transform(y_train), num_classes=len(le.classes_))\n",
        "y_test_encoded  = to_categorical(le.transform(y_test), num_classes=len(le.classes_))"
      ],
      "metadata": {
        "id": "JsbUQdK2pNrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K8ZpsMWpsfQ",
        "outputId": "3bf140a3-ffa4-4945-e781-cbaac64a594f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N',\n",
              "       'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "dnQStB-jxVzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "d2MOYY18xMLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "StLU0Lpoxbr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index)+1, 20, input_length=50))\n",
        "model.add(Conv1D(32,8))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(32,4))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len=(le.classes_), activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='c')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7rOJYB6p645",
        "outputId": "1c87632f-b984-4949-d7d7-6510b749ec01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " token_and_position_embeddin  (None, 50, 20)           1440      \n",
            " g_2 (TokenAndPositionEmbedd                                     \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_block_3 (Transf  (None, 50, 20)           15596     \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 50, 32)            672       \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 50, 32)            0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 50, 21)            693       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,401\n",
            "Trainable params: 18,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train_tokens_padded,\n",
        "    y_train_encoded,\n",
        "    validation_data=(\n",
        "        X_test_tokens_padded,\n",
        "        y_test_encoded\n",
        "    ),\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "ssE4c6EwrHvz",
        "outputId": "f2c89ed6-6e65-4f66-d76e-38537bd08bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d4404a71d07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX_train_tokens_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     validation_data=(\n\u001b[1;32m      5\u001b[0m         \u001b[0mX_test_tokens_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 21) and (None, 50, 21) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sample(array, temperature=1.0):\n",
        "  array = np.log(array) / temperature\n",
        "  array = np.exp(array) / np.sum(np.exp(array))\n",
        "  return np.argmax(np.random.multinomial(1, array, 1))\n",
        "\n",
        "def generate_peptide(max_length=50):\n",
        "  sequence = '$'\n",
        "  next_token = None\n",
        "  while len(sequence) + 1 < max_length and next_token != \"@\":\n",
        "    sequence_tokens = tokenizer.texts_to_sequences([sequence])\n",
        "    sequence_tokens_padded = pad_sequences(sequence_tokens, maxlen=max_length)\n",
        "    y_pred = model.predict(sequence_tokens_padded, verbose=False)[0]\n",
        "    next_token = le.inverse_transform([sample(y_pred)])[0]\n",
        "    sequence += next_token\n",
        "  return sequence.strip('$').strip('@')\n",
        "\n",
        "for i in range(100):\n",
        "  anticancer_peptide = generate_peptide()\n",
        "  print(f'>{i}')\n",
        "  print(generate_peptide())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC28zsU5sViW",
        "outputId": "285e7822-845e-4150-e32c-6ddc9c771fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">0\n",
            "KAAAILEKFVKKVL\n",
            ">1\n",
            "LKCAPHGEAAGFGPI\n",
            ">2\n",
            "WLF\n",
            ">3\n",
            "ILKKLGKKLSLFALIHVLPKLKTAKK\n",
            ">4\n",
            "NGFFGLFKSIWKTAGCFIRELFLHKIVQANRKPERKGA\n",
            ">5\n",
            "ILVKGALGLKAALAKFLAKKAAKKLG\n",
            ">6\n",
            "RTSGRCSGVLPALGVCSMCHHILGTFGCARKRCSLRQYR\n",
            ">7\n",
            "SKK\n",
            ">8\n",
            "EGDKSPFGSRLFCGRIKG\n",
            ">9\n",
            "LLAKIIKWLVKHLA\n",
            ">10\n",
            "FFAKLIKKLAKLAKKAVK\n",
            ">11\n",
            "ALQGSLALGNAGSGKLFVGKCRNMVTWGLCKTPIDCKNYIHSGLWVHE\n",
            ">12\n",
            "LAFLLKALKKAA\n",
            ">13\n",
            "HIEVADYRCG\n",
            ">14\n",
            "GTLFLLIKKI\n",
            ">15\n",
            "LKLLKKALAAFFKLAK\n",
            ">16\n",
            "FLFHPFLLTHNPPFL\n",
            ">17\n",
            "GENTGLKYDLQRACYGGFLS\n",
            ">18\n",
            "VFFAKLSCKLKCRSCQPNPKWIKMCHLRPKDRRYDS\n",
            ">19\n",
            "GALDCILTVCGKQDISACALPCDGHCHRRKKGNY\n",
            ">20\n",
            "DLLLNLLKVFA\n",
            ">21\n",
            "IVGSDIN\n",
            ">22\n",
            "KFKFLWKALLKLL\n",
            ">23\n",
            "WAGFSCAFLCKKLT\n",
            ">24\n",
            "PRYGADFWGGVREKNCILESKWHPI\n",
            ">25\n",
            "D\n",
            ">26\n",
            "ITTVLLAALHIANVVL\n",
            ">27\n",
            "CKGLKKIAKFI\n",
            ">28\n",
            "AAPAKIVAGPGIDFTG\n",
            ">29\n",
            "FLF\n",
            ">30\n",
            "IAHLGCGAACRNP\n",
            ">31\n",
            "TWAAALKLFNGI\n",
            ">32\n",
            "DCAHGSPIEYWLGDCKVSKDGTMYAYPRKRCSWLHTT\n",
            ">33\n",
            "AKFLKKALKAAK\n",
            ">34\n",
            "LWPLWAPALKKLASKA\n",
            ">35\n",
            "AKGWGKAFKKALKKA\n",
            ">36\n",
            "\n",
            ">37\n",
            "LAKRFPKAKLLLKAAK\n",
            ">38\n",
            "WSAIKTRPCAVTTVNGPKHGSVTCDGSTKGVYRVGGKCGERVRH\n",
            ">39\n",
            "WGKAGKFIGLAGKAA\n",
            ">40\n",
            "FLLILKRKIDKAAARLTFAKT\n",
            ">41\n",
            "PGIDQNNLGYGLNHPSWSGEAKARHAKNAFCDAGPKCPIPC\n",
            ">42\n",
            "GQKS\n",
            ">43\n",
            "TNFLLLKIDKW\n",
            ">44\n",
            "PFKPMWFPV\n",
            ">45\n",
            "TQKAPRQLI\n",
            ">46\n",
            "IPKKCVYCKKSYT\n",
            ">47\n",
            "WIRRMRCKWAGVYVKAA\n",
            ">48\n",
            "KTLFPPKSPPTGNGRDP\n",
            ">49\n",
            "GLLKLIGGFACKR\n",
            ">50\n",
            "PAVGLIAGGALKHANKLLNNIKVMAPTLPPGIWH\n",
            ">51\n",
            "PIDKIKTLFKKALHA\n",
            ">52\n",
            "MLLKAKKKFEAF\n",
            ">53\n",
            "LFKKKKKKGIRPCSVIYFH\n",
            ">54\n",
            "KSKWRGKKF\n",
            ">55\n",
            "RKAAHGVVKLHRHMAQSMKKAKWKKQNKLKRKLANSGQKAGSGAIGKL\n",
            ">56\n",
            "KVWDLIRRRMRQLEPRPSGNHKQGGRYIFKDPVNRRDLTAGCGIPKGQ\n",
            ">57\n",
            "WFAWVKLLAKALH\n",
            ">58\n",
            "TTWGPLAMGGVHLDTIAS\n",
            ">59\n",
            "QQKKHLFKKIGVK\n",
            ">60\n",
            "LGKAKAVLLFLAALLKKL\n",
            ">61\n",
            "WLLKLLKKWLKKLL\n",
            ">62\n",
            "WFAFLKKLKKAVVLLLLAL\n",
            ">63\n",
            "HKEPTQKLNIHCQE\n",
            ">64\n",
            "FAVAWLFKAKKKFL\n",
            ">65\n",
            "LFAKLWKKLLKKLLKKKAKKK\n",
            ">66\n",
            "LAKGAIKRAVHKFL\n",
            ">67\n",
            "ALKKAKKFWKKAA\n",
            ">68\n",
            "KLKIALKKIIKKA\n",
            ">69\n",
            "FWAALKKLW\n",
            ">70\n",
            "VGKEALHNAHSRKKAWKYAMSCKFGTKYTLVC\n",
            ">71\n",
            "WPTPVFWDQFGRVRARGEMKRWEFPGEY\n",
            ">72\n",
            "HVFLKIPFVIAG\n",
            ">73\n",
            "KKKLWWKTVWNDIPSQFDCMAMYEN\n",
            ">74\n",
            "KVKKKAGKALKRLAA\n",
            ">75\n",
            "FIGKLLGLLKKLLCAATSLKKAKKS\n",
            ">76\n",
            "IFKAFIKKLAKAAKKLAK\n",
            ">77\n",
            "FRESDSNTLTNFVHTKKGANLLL\n",
            ">78\n",
            "WKFADRLKCKPCTKYGKWKCCGETQKQGCHYACGFGLCHLCCCAWCH\n",
            ">79\n",
            "SLDDGFGVRVGLGP\n",
            ">80\n",
            "MFALAFWKAKGKAVK\n",
            ">81\n",
            "FIIAEDLHSRLSKQLW\n",
            ">82\n",
            "TEYLHAGQCPDFVLGEAALPSAAWVIIRHLK\n",
            ">83\n",
            "CVFVTAF\n",
            ">84\n",
            "VKKGGLVPLNGCKGLLQIRK\n",
            ">85\n",
            "KLQRTWIKHWKKCHKSGPC\n",
            ">86\n",
            "KLLAVILKVIKLALKLL\n",
            ">87\n",
            "VVSFLINFPDF\n",
            ">88\n",
            "YLGRLLKKLAWLTVDIQFS\n",
            ">89\n",
            "IGFLGSLGKLIHKFRGLFKKQKT\n",
            ">90\n",
            "VPDWWTGIYKKK\n",
            ">91\n",
            "A\n",
            ">92\n",
            "LAWKLLRKIKANK\n",
            ">93\n",
            "KLVAFWKGFKRWGVVMLLNL\n",
            ">94\n",
            "SR\n",
            ">95\n",
            "HCNELPCWEGPWMGCDEPMPSNSADRVHPVCFLCAACDHSG\n",
            ">96\n",
            "WHSQQYENSTGFDWDRKLCKNSGAVHFPFYGFLE\n",
            ">97\n",
            "FKGALKKALKKVLK\n",
            ">98\n",
            "NVTVLM\n",
            ">99\n",
            "FKGGLWFLFPIFPITSHV\n"
          ]
        }
      ]
    }
  ]
}